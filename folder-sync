#!/usr/bin/env zsh
# use bash explicitly to avoid issues arround printf, declare -n, etc
(set -o igncr) 2>/dev/null && set -o igncr; # cygwin specific shell options

#     ____     __   __        ____
#    / __/__  / /__/ /__ ____/ __/_ _____  ____
#   / _// _ \/ / _  / -_) __/\ \/ // / _ \/ __/
#  /_/  \___/_/\_,_/\__/_/ /___/\_, /_//_/\__/
#                              /___/

## set trap commands
#trap clean SIGINT SIGTERM
#trap "exit" INT TERM ERR
#trap "kill 0" EXIT # terminated all jobs on exit

###########################
# functions
###########################

function catch_stderr_2() {
    local -n v="$1";
    shift && { v="$("$@" 2>&1 1>&3 3>&-)"; } 3>&1;
    echo "$v"
}

function catch_stderr() {
    local err=''
    { err="$("$@" 2>&1 1>&3 3>&-)"; } 3>&1

    if [[ "$err" != '' ]]
    then
        echo "$err"
        return 1
    else
        return 0
    fi
}

function usage() {
    [[ "$1" = "" ]] || echo -e "\n\t${r}ERROR: \"$1\" is a required argument but is missing or is invalid${s}"
    echo -e "Usage: folder-sync start | stop [options]

        Options:
        <-s | --source>  <-d | --dest>  <-u | --user>  <-p | --password>
        [-m | --marker] [-R] [-v] [-q] [-b]
        Type 'folder-sync --help' for more information."
    exit 2
}

function help {
   echo -e "${s}

    Description:
        Get directory listing from an FTP server and download all new files as per the provided options.

        Note -- there are three options for identifying \"new\" files:
            1. Any file newer that the last file downloaded in the destination directory
            2. Any file newer than the script start time if there are no files in the destination directory
            3. Any file newer than the time marker provided with the -m option e.g. -m \"yesterday\"
                see GNU documentation for all accepted time formats:
                https://www.gnu.org/software/coreutils/manual/html_node/Date-input-formats.html#Date-input-formats

        To continuously download new files, use the -b option

    Commands:
        start   execute the script
        stop    stop the background process (applicable only for -b option)

    Required:
        -s , --source     set the FTP source URL along with the path i.e. \"ftp://192.168.1.1/ftp/root/files\"
        -d , --dest       set the destination directory
        -u , --user       set user name

    Optional:
        -p , --password   set the password. If omitted, the script will ask for it on the console
        -f , --file       specific file to download, wildcard (*) pattern accepted
        -m , --marker     custom cut-off time marker
        -l , --logfile    log file
        -L , --level      specify log level from 0 to 4: (0) silent; (1) error; (3) warn; (2) info; (4) verbose
        -R                after download completes, delete the file from the source location
        -O                force overwrite when file with the same name already exists
                              (by default will overwrite only when the files have same size and date)
        -k                never overwrite when file with the same name already exists
        -v                show verbose output
        -q                don't show any output
        -b                run in the background"
    exit 0
}

# shellcheck disable=SC2153
# shellcheck disable=SC2034
parse_args() {
    declare -n cmdargs=$1
    declare -n argsmap=$2

    local flags="kOhvqRb:s:d:u:p:m:l:L:f:"
    local long="start,stop,help,source:,dest:,user:,password:,marker:logfile:,level:,file:"
     echo "******"
    declare -p cmdargs
    echo "cmdargs: ${cmdargs[@]}"
    parsed=$(/usr/bin/getopt -a -n folder_sync -o "$flags" --long "$long" -- "${cmdargs[@]}")
    [[ $? -ne 0 ]] && usage

    eval set -- "$parsed"
    while :
    do
      case "$1" in
        # flags
        -R ) argsmap[REM_FILE]="y"     ; shift ;;
        -O ) argsmap[KEEP]="n"         ; shift ;;
        -k ) argsmap[KEEP]="y"         ; shift ;;
        -v ) argsmap[LEVEL]=4          ; shift ;;
        -q ) argsmap[LEVEL]=0          ; shift ;;
        --start ) argsmap[CMD]="START" ; shift ;;
        --stop ) stop ;;

        # options
        -h | --help) help ;;
        -s | --source)   argsmap[URL]=$2     ; shift 2 ;;
        -f | --file)     argsmap[FILE]=$2    ; shift 2 ;;
        -d | --dest)     argsmap[OUT_DIR]=$2 ; shift 2 ;;
        -u | --user)     argsmap[USER]=$2    ; shift 2 ;;
        -p | --password) argsmap[PWD]=$2     ; shift 2 ;;
        -l | --logfile)  argsmap[LOG]=$2     ; shift 2 ;;
        -L | --loglevel) argsmap[LEVEL]=$2   ; shift 2 ;;
        -m | --marker)   argsmap[MARKER]=$(date -d "$2" '+%s'); shift 2 ;;
        -b ) argsmap[SERVICE]=1; argsmap[SLEEP]=$2; shift 2 ;;
        --) shift; break ;;
        * ) echo "Unexpected option: $1" ; usage ;;
      esac
    done
}

function stop {
    if [[ "$(uname)" = "Linux" ]]
    then
        killall -r 'folder-sync' 2>&1
    else
        #cygwin
        CYG_PID="$(procps -wwFAH | grep -v "grep" | grep "folder-sync"  | tr -s ' ' | cut -d ' ' -f 2)"
        echo -I -e "Stopping CYGWIN PID(s)"
        echo "$CYG_PID"
        kill -s SIGTERM "$CYG_PID" 2>&1 /dev/null
    fi
    exit
}

#Cleanup on kill
function clean() {
    echo -F -e "Download failed! $1"
    sleep 1 # give it a chance to display the error before exiting
    kill 0
    rm -Rf "$OUT_DIR/wrk"
    exit 1
}

function run_job() {
    local job_pid=$1
    ({
        local err
        shift && { err="$("$@" 2>&1 1>&3 3>&-)"; } 3>&1;
        echo "$err"
        [[ "$err" != '' ]] && { echo "$err"; return 1; }
    }  ) &

    eval "$job_pid"="$!"
    return
}

function read_password() {
    local pwd="$1"
    local password

    echo -n "Enter password: "
    while IFS= read -p "$prompt" -r -s -n 1 char
    do
        if [[ $char == $'\0' ]] ; then break; fi

        # handle backspace
        if [[ $char == $'\177' ]] ; then
            prompt=$'\b \b'
            password="${password%?}"
        else
            prompt='*'
            password+="$char"
        fi
    done

    eval "$pwd"="$password"
    echo
}

# override built-in echo to customize logging
function echo() {
    if [[ "$1" =~ ^(-[IDWF]) ]]; then log "$@"; else builtin echo "$@"; fi #| tee -a "$LOG_FILE_NAME" 2>&1 ; fi
}

function log() {
    local echo_cmd="builtin echo"
    local severity="$1" && shift

    (( LEVEL == 0 )) && return 0

#    [[ $1 =~ (^-[-neE]) ]] && echo_cmd="builtin echo ${BASH_REMATCH[1]} $stamp " && shift
    while :
    do
        if [[ $1 =~ (^-[-neE]) ]]; then
            echo_cmd=" $echo_cmd ${BASH_REMATCH[1]}" && shift
        else
            break;
        fi
    done

    # only add timestamp at certain levels (needed?)
    (( LEVEL >= 4 )) && stamp="$(date +"[%m-%d %H:%M:%S]")"
    echo_cmd="$echo_cmd $stamp"

    case $severity in
        -D ) (( LEVEL >= 4 )) && $echo_cmd " $*" ;; # print everything
        -I ) (( LEVEL >= 3 )) && $echo_cmd " $*" ;; # print info
        -W ) (( LEVEL >= 2 )) && $echo_cmd " $*" ;; # print warnings
        -F ) (( LEVEL >= 1 )) && $echo_cmd " $*" ;; # print errors
        *  ) (( LEVEL >= 1 )) && $echo_cmd " $*" ;; # print errors by default
    esac

    #log to file everything but without any escape characters
    printf "%s\n" "$*" | sed 's/[\x01-\x1F\x7F]//g' >> "$LOG_FILE_NAME" 2>&1
}

# returns the last modified time for the file provided in the URL
function get_mtime() {
    local mtime
    mtime=$(curl -s -I -u "$USER:$PWD" "$1" 2>&1 | awk '/Last-Modified:/ {sub("\r",""); print  $2,$3,$4,$5,$6,$7}')
    echo "$(date -d "${mtime}" '+%s')" # convert to ms
}

function get_marker() {
    local latest=$(find "$1" -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -d ' ' -f 1)

    if [[ -z ${latest+_} ]]; then
      echo "$(date '+%s')"
    else
      echo "$(date -d "@$latest" '+%s')"
    fi
}


# filters out any files that are older than the cutoff time
function filter_mtime() {
    local -n newer=$1
    local url=$2
    local cutoff=$3

    local lmd
    # shellcheck disable=SC2068  # gobbling needs to be enabled for the loop
    for f in ${newer[@]}; do
        lmd=$(get_mtime "$url/$f")
        if [[ -z "$lmd" ]]; then
            echo -W -e "Could not determine the last modification time for $f. Will not download!"
            continue
        fi

        if [[ "$cutoff" -ge "$lmd" ]]; then
            echo -D -e "(-) $f is old - skipped"
            newer=("${newer[@]/$f}") # remove old file
        else
            echo -D -e "(+) $f is newer - downloading"
        fi
    done

    echo -D "Found ${#newer[@]} files newer than [$(date -d @"$cutoff")]"
    return 0
}

function filter_filename(){
    local -n matched=$1
    local pattern=$2

    # shellcheck disable=SC2068  # gobbling needs to be enabled for the loop
    for f in ${matched[@]}; do
        if [[ $f =~ $pattern ]]; then
            echo -D -e "(+) $f name matches - downloading"
        else
            echo -D -e "(-) $f name does not match - skipping"
            matched=("${matched[@]/$f}") # remove old file
        fi
    done
}

# get the file details and parse the size
function file_get_size() {
    local size
    size="$(curl -u "$USER:$PWD" -qIL "$1" 2> /dev/null | awk '/Length/ {print $2}' | grep -o '[0-9]*')"
    echo "${size:-0}"
}

function file_move() {
    local file_name="$1"
    local source_dir="$2"
    local target_dir="$3"

    [[ ! -f "$source_dir/$file_name.1" ]] && return 1

    #Join all the parts # todo preserve the modification times
#    eval cat "$source_dir/$file_name".{1..1} > "$source_dir/$file_name.completed"
#    rm -f "$source_dir/$file_name".{1..1}
    mv "${source_dir}/${file_name}.1" "${source_dir}/${file_name}.completed" > /dev/null 2>&1

    # just move it if there is no name conflict
    if  [[ "$KEEP" = 'n' ]] || [[ ! -f "$target_dir/$file_name" ]]; then
        err_msg=$(catch_stderr mv "${source_dir}/${file_name}.completed" "$target_dir/$file_name")
        [[ $? != 0 ]] && { echo "$err_msg"; return 2; }
        return 0
    fi

    # rename per the options provided
    local overwrite=false

    # if option set to 'auto' check for matching name, size, mod date and overwrite
    if (( $(stat -c%s "$source_dir/$file_name.completed") == $(stat -c%s "$target_dir/$file_name") )); then
        local same_size=true
    fi

    local new_file_date="$(date -d "$(stat "$source_dir/$file_name.completed" | awk '/Modify/ {print $2" "$3" "$4}')")"
    local curr_file_date="$(date -d "$(stat "$target_dir/$file_name" | awk '/Modify/ {print $2" "$3" "$4}')")"
    if [[ "$new_file_date" = "$curr_file_date" ]] ; then
        local same_date=true
    fi

    if [[ $same_size = "true" ]] && [[ $same_date = "true" ]]; then overwrite=true; fi

    local dest_name=$file_name

    # if option set to 'never' generate new name
    if [[ "$KEEP" = 'y' ]] || [[ $overwrite = "false" ]] ; then
        local i=1
        while [[ -f "$target_dir/$dest_name" ]]; do
            dest_name="${file_name%.*}($(( i++ ))).${file_name##*.}"
        done
    fi

    # move to the final destination using the new file name
    err_msg=$(catch_stderr mv "${source_dir}/${file_name}.completed" "$target_dir/$dest_name")
    [[ $? != 0 ]] && { echo "$err_msg"; return 2; }

    return 0
}

function file_remove() {
    local filepath="/$(echo "$2" | grep / | cut -d/ -f4-)"
    curl --ftp-pasv -s -u "$1" "$2/" -Q "DELE $filepath/$3" 2> /dev/null
}

function file_download() {
    local src_dir="$1"
    local dest_dir="$2"
    local file_name="$3"
    local size="$4"

    local chunks=1 # only needed when downloading in multiple parts
    local wrk_dir="$dest_dir/.wrk"

    mkdir -p "$wrk_dir"; chmod 755 "$wrk_dir"
    [[ -d "$wrk_dir" ]] || {
        echo -e "ERROR: Cannot create working directory [$wrk_dir]. Please check permissions";
        exit;
    }

    local origname=$file_name
#    [[ $file_name == 'BCDriverMerge.exe' ]] && file_name='*BCDriverMerge.exe' # just for testing
#    [[ $file_name == 'BCWim.bin' ]] && file_name='*BCWim.bin' # just for testing
    ( {
    sleep 5;
    curl --write-out "%{http_code}" -sS -R --ftp-pasv \
        -u "$USER:$PWD" -o "${wrk_dir}/${file_name}.1" "$src_dir/$file_name" >/dev/shm/$origname  2>&1; } ) &

#    wait_for_completion "Processing" "$wrk_dir" "$file_name" $chunks "$size"
}

function wait_for_completion() {
    local action="$1" #
    local directory="$2"
    local file_name="$3"
    local chunks="$4"
    local total_size="$5"

    # Wait for the job to complete while showing progress
    local time=$(( $(date +%s)-1 ))
    while jobs | grep -q Running ;
    do
        sleep 1
        track_progress "Processing" "$wrk_dir" "$file_name" "$chunks" "$size" "$time"
        tput ech ${#size} # erase the last reported size value
        tput cub 1000 # move 1000 chars left
    done

    read -r job_response < /dev/shm/fs # get the job actual output
    job_err=$(jobs | awk '/Exit/ {print $3}') # or another way to get just the exit code

    if [[ "$job_err" == '' ]]; then
        track_progress 'Processed' "$wrk_dir" "$file_name" "$chunks" "$size" "$time"
    else
        track_progress 'Canceled' "$wrk_dir" "$file_name" "$chunks" "$size" "$time" "$job_response"
        return 1
    fi

#    err_msg=$(catch_stderr file_move "$file_name" "$wrk_dir" "$dest_dir")
#    [[ $? != 0 ]] && { echo "$err_msg"; return 1; }
}

# takes an array reference and updates it with the file names in the given directory
function dir_get_content() {
    local -n arr=$1
    local url="$2"

    local curl_out
    curl_out=$(curl -sS -u "$USER:$PWD" "$url/" 2>&1)
    curl_err=$?

    if [[ $curl_out =~ (^curl.*) ]]; then
        echo -F -e "Failed to retrieve directory content. Error message: ${BASH_REMATCH[1]}"
        return $curl_err
    fi

    # shellcheck disable=SC2034
    mapfile -t arr < <(echo "$curl_out" | \
        sed -nr 's/(^-[ rw-].*)( *)([0-9])(.*)/\4/p' | sed -e 's/^ //g' | sed -e 's/ /\ /g' )

    return 0
}

function dir_scan() {
    local url="$1"
    local time="$2"
    local out="$3"
    local -a files
    declare -A sz_map
    echo -I -e "Checking [$url] for files newer than [${y}$(date -d @"$time")${s}]"

    dir_get_content files "$url"
    if [[ $? -ne 0 ]]; then clean "Failed to get directory content"; fi

    filter_mtime files "$url" "$time"
    if [[ $? -ne 0 ]]; then clean "Failed to identify newer files"; fi

    filter_filename files "${ARGS_MAP[FILE]:-.*}"

    [[ -n "${files[*]}" ]] || { echo -W -e "No new files"; eturn 0; }

    echo -D -e "Downloading the following files:"
    # shellcheck disable=SC2068  # gobbling needed here
    for f in ${files[@]}; do echo -D "$f"; done

    # shellcheck disable=SC2068
    for f in ${files[@]}; do
        local size=$(file_get_size "$url/$f")
        sz_map[$f]=$size
        file_download "$url" "$out" "$f" "$size"

#    err_msg=$(catch_stderr file_move "$file_name" "$wrk_dir" "$dest_dir")
#    [[ $? != 0 ]] && { echo "$err_msg"; return 1; }

#        if [[ $? -ne 0 ]]; then
#            ((ERR_FLAG++))
#            #echo -E -e "${y}Error reported when downloading file $f${s}"
#        elif  [[ "$DELETE_REMOTE_FILE" = "y" ]]; then
#            err_msg=$(file_remove "$USER:$PWD" "$url" "$f")
#            [[ $? != 0 ]] && { ((ERR_FLAG++)); echo -E -e "Error reported when removing remote file $f [$err_msg]"; }
#        fi
    done

    # Wait for the job to complete while showing progress
    local files_num=${#files[@]}
    local time=$(( $(date +%s)-1 ))
    while jobs | grep -q Running ;
    do
        sleep 1
        for K in "${!sz_map[@]}"; do
            fn=$K
            fs=${sz_map[$K]}
            # consider using watch -n 5 free -m
            track_progress "Processing" "$out/.wrk" "$fn" "1" "$fs" "$time"
            tput el
            echo
        done
        tput ech ${#fs} # erase the last reported size value
        tput cub 1000 # move 1000 chars left
        echo -en "\033[${files_num}A" # move the cursor up N lines: \033[<N>A
    done

    for K in "${!sz_map[@]}"; do
        fn=$K
        fs=${sz_map[$K]}
        read -r job_response < /dev/shm/$fn # get the job actual output

        if [[ "$job_response" == '' ]] || [[ "$job_response" =~ '226' ]]; then
            track_progress 'Processed' "$out/.wrk" "$fn" "1" "$fs" "$time"
            file_move "$fn" "$out/.wrk" "$out"
        else
            ((ERR_FLAG++))
            track_progress 'Canceled' "$out/.wrk" "$fn" "1" "$fs" "$time" "$job_response"
        fi
    done
}

function wait_for_all() {
    local action="$1" #
    local directory="$2"
    local file_name="$3"
    local chunks="$4"
    local total_size="$5"
    local -a all_files=$6
    declare -A sz_map

    for f in ${all_files[@]}; do
        sz_map[f]=$(wc -c "$directory/$f" | cut -d ' ' -f 1)
    done

for K in "${!all_files[@]}"; do echo -D "[$K] : [${all_files[$K]}]"; done
#    # Wait for the job to complete while showing progress
#    local time=$(( $(date +%s)-1 ))
#    while jobs | grep -q Running ;
#    do
#        sleep 1
#        track_progress "Processing" "$wrk_dir" "$file_name" "$chunks" "$size" "$time"
#        tput ech ${#size} # erase the last reported size value
#        tput cub 1000 # move 1000 chars left
#    done
#
#    read -r job_response < /dev/shm/fs # get the job actual output
#    job_err=$(jobs | awk '/Exit/ {print $3}') # or another way to get just the exit code
#
#    if [[ "$job_err" == '' ]]; then
#        track_progress 'Processed' "$wrk_dir" "$file_name" "$chunks" "$size" "$time"
#    else
#        track_progress 'Canceled' "$wrk_dir" "$file_name" "$chunks" "$size" "$time" "$job_response"
#        return 1
#    fi
#
#
#    err_msg=$(catch_stderr file_move "$file_name" "$wrk_dir" "$dest_dir")
#    [[ $? != 0 ]] && { echo "$err_msg"; return 1; }
}

function track_progress() {
    local action="$1"
    local directory="$2"
    local file_name="$3"
    local chunks="$4"
    local total_size="$5"
    local time="$6"
    local err="$7"
    local Kb=1024; local Mb=1048576; local Gb=1073741824; local units=$Kb; local scale="Kb"

    [[ total_size -eq 0 ]] && return
    [[ total_size -ge $Mb ]] && { units=$Kb; scale="Kb"; }
    [[ total_size -ge $Gb ]] && { units=$Mb; scale="Mb"; }
    [[ total_size -gt $Gb ]] && { units=$Gb; scale="Gb"; }

    local completed_size=$(( $(eval ls -l "$directory/$file_name.{1..$chunks}" 2> /dev/null | awk 'BEGIN{s=0}{s+=$5}END{print s}') ))
    local elapsed=$(( $(date +%s) - time ))
    local rate=$(( ( completed_size / elapsed )/units ))
    local pct=$(( ( completed_size*100 )/total_size ))

    local status
    if [[ $pct -lt 100 ]]; then status="${y}Downloading${s}"; else status="${g}Completed${s}"; fi
    if [[ $action == 'Canceled' ]]; then status="${r}FAILED${s}"; fi

    local msg
    msg+=$(printf "%-15b" "$action")
    msg+=$(printf "%-30s" "$file_name")
    msg+=$(printf "%6s/%-6s%s @%6s %3s/sec (%3s%%)" $((completed_size/units)) $((total_size/units)) $scale $((rate)) $scale $pct)
    msg+=$(printf "%-25b" "......$status")
    if [[ $action == 'Canceled' ]]; then msg+=$(printf "%40s" "[$err]"); fi
    echo -I -n "$msg"

    # end status line when download has completed
    [[ $action == 'Processed' ]] && echo -I
    [[ $action == 'Canceled' ]] && echo -I
}

function main {
   echo "TBD"
}

###########################
# main
###########################
ERR_FLAG=0
# generate log file suffix as preferred
# printf -v LOG_FILE_NAME_SUFFIX "%(%Y%m%d-%H%M%S-)T$(( RANDOM % 1000 ))" -1
printf -v LOG_FILE_NAME_SUFFIX "%s" $(date +%Y_%m_%d)

###########################
# parse args
###########################

# shellcheck disable=SC2034
CMD_ARGS=("$@")

declare -p CMD_ARGS
echo "cmdargs: ${CMD_ARGS[@]}"

declare -A ARGS_MAP
parse_args CMD_ARGS ARGS_MAP

# validate required arguments
[[ ${ARGS_MAP[URL]+_} ]] ||  usage "-s | --source"
[[ ${ARGS_MAP[USER]+_} ]] || usage "-u | --user"
[[ ${ARGS_MAP[OUT_DIR]+_} ]] || usage "-d | --dest"

# validate destination
OUT_DIR=${ARGS_MAP[OUT_DIR]}
if [[ ! -d "$OUT_DIR" ]]; then mkdir -p "$OUT_DIR"; chmod 0755 "$OUT_DIR"; fi
[[ -d "$OUT_DIR" ]] || usage "-d | --dest"

# set log
LOGS_ROOT="$OUT_DIR/.log" && mkdir -p "$LOGS_ROOT"
LOG_FILE_NAME=${ARGS_MAP[LOG]:-"${LOGS_ROOT}/folder-sync.${LOG_FILE_NAME_SUFFIX}.log"}
LEVEL=${ARGS_MAP[LEVEL]:-3}

for K in "${!ARGS_MAP[@]}"; do echo -D "[$K] : [${ARGS_MAP[$K]}]"; done

URL="${ARGS_MAP[URL]}"
USER="${ARGS_MAP[USER]}"
DELETE_REMOTE_FILE=${ARGS_MAP[REM_FILE]:-"n"}
KEEP=${ARGS_MAP[KEEP]:-"a"} # (y)es / (n)o / (a)uto -> i.e. rename only if same size & date
CMD=${ARGS_MAP[CMD]:-"START"}

# use the provided time marker, latest timestamp in the destination directory or current time (in this order)
CUT_OFF_TIME=${ARGS_MAP[MARKER]:-$(get_marker "$OUT_DIR")}

# set password
PWD="${ARGS_MAP[PWD]}"
[[ -n "$PWD" ]] || read_password PWD

# set service options
SERVICE=${ARGS_MAP[SERVICE]:-0}
SLEEP=${ARGS_MAP[SLEEP]:-60}

# ansi colors
r='\033[1;31m' ; g='\033[1;32m'; y='\033[1;33m'; s='\033[0;37m'

read -r -d '' LOGO <<- EOM
\n
    ${g}   ____     __   __       ${y}   ____
    ${g}  / __/__  / /__/ /__ ____${y}  / __/_ _____  ____
    ${g} / _// _ \/ / _  / -_) __/${y}  \ \/ // / _ \/ __/
    ${g}/_/  \___/_/\_,_/\__/_/   ${y}/___/\_, /_//_/\__/
    ${g}                          ${y}    /___/
EOM
echo -I -e "$LOGO${s}"

if [[ "$SERVICE" -eq 1 ]]
then
    TIME_MARKER="$CUT_OFF_TIME"
    # setsid fs.sh >/dev/null 2>&1 < /dev/null & # run as daemon
    while /bin/true; do
        dir_scan "$URL" "$TIME_MARKER" "$OUT_DIR"
        sleep "$SLEEP"
        TIME_MARKER=$(get_marker "$OUT_DIR") #update time marker
    done >> "$LOG_FILE_NAME" 2>&1 &
    echo -e "${g}Folder-sync started in background; use '--stop' to terminate${s}" | tee -a "$LOG_FILE_NAME" 2>&1
else
    echo -I -e "${g}Folder-sync running...${s}"
    dir_scan "$URL" "$CUT_OFF_TIME" "$OUT_DIR" #| tee -a "$LOG_FILE_NAME" 2>&1
    if [[ $ERR_FLAG -eq 0 ]]
    then
        echo -I -e "${g}Folder-sync completed${s}" #| tee -a "$LOG_FILE_NAME" 2>&1
    else
        echo -W -e "${y}Folder-sync completed with errors${s}" #| tee -a "$LOG_FILE_NAME" 2>&1
    fi
    rm -Rf "$OUT_DIR/.wrk" | tee -a "$LOG_FILE_NAME" 2>&1
fi

