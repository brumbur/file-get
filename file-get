#!/bin/bash


#  1 - от къде да сваля
#  2 - къде да качва  (temp) или в папка на друга машина в мрежата
#  3 - ключ дали да трие сваленото (y/n)
#
#  - време не ни трябва, но разбира се не трябва да сваля файл 2 пъти освен ако не е нов файл със същото име
#  - възможнжст за стартиране на повече от един скрипт едновремено
#  - като екстра за напреднали - да проверява за наличието на стринг в името на файла. Например " BG60" или "BG 60" или "BG_60". Ако има такъв стринг - да качи файла в друга папка (temp1)
#
#  $ ./file-get ftp://pesho:mzprx@192.168.1.1:21/remote/directory ./temp  y/n ("BG60","BG 60","BG_60") ./temp1


###########################
# functions
###########################

# monitor given remote directory and download new files to the target directory
function usage {
  echo -e "Usage:\n\t`basename $0` [source url] [target dir] [cutoff time] [chunk size (default: 128MB)]"
  echo -e "Example:\n\t `basename $0` ftp://pesho:mzprx@192.168.1.1:21/remote/directory /tmp \"Mon, 17 Feb 2020 20:38:04 GMT\""
  echo -e "\n\t `basename $0` ftp://pesho:mzprx@192.168.1.1:21/remote/directory /tmp yesterday"
  exit 1
}

#Cleanup on kill
function clean {
  echo "Download failed! Cleaning..."
  # kill 0
  rm -R $OUT_DIR/wrk 2> /dev/null
  rm $OUT_DIR/$FILENAME.* 2> /dev/null
  exit 1
}

# get the file details and parse the size
function getSize() {
  local SIZE="`curl -qIL $1 2> /dev/null | awk '/Length/ {print $2}'|grep -o '[0-9]*'`"
  local SIZE=${SIZE:-1}
  echo $SIZE
  return
}

# takes an array reference and updates it with the file names in the given directory
function getDirectoryContent() {
  local -n arr=$1
  local url="$2/"
  echo "url $2"
  local response=`curl -s "$url"`

  if [ $? -eq 0 ]
  then
    echo ""
  else
    echo "ERROR: $?" >&2
    clean
  fi

  arr=($(curl -s "$url" | sed -nr 's/(^-[ rw-].*)( *)([0-9])(.*)/\4/p' | sed -e 's/^ //g' | sed -e 's/ /\ /g'))

  return
}

# takes the complete URL as argument and returns the last modified time
function getSourceLastModifiedDate() {
    local dt=`curl -I "$1" 2>&1 | awk '/Last-Modified:/ {sub("\r",""); print  $2,$3,$4,$5,$6,$7}'`
    local ms="$(date -d "${dt}" '+%s')" # convert to ms
    echo "$ms"
    return
}

function getDestinationLastModifiedDate() {
    local destDir=$1
    local last=`find $destDir -type f -printf '%T@ %p\n' | sort -n | tail -1`
    file=(${last// / })
    echo $(date -d "@${file[0]}" '+%s')
    return
}

find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d" "

# filters out any files that are older than the cutoff time
function getNewFiles() {
    local -n arr=$1
    local url=$2
    local cutoff=$3

    for f in ${arr[@]}; do
        local lmd=$(getSourceLastModifiedDate "$url/$f")
        if [ $cutoff -gt $lmd ]; then
            echo "$f is old, skipping it"git c
            arr=("${arr[@]/$f}") # remove old file
        fi
    done
    return
}

function calcProgress() {
  # local FNAME=$1
  # local PARTNUM=$2
  # local SZ=$3
  # local T=$4

  [[ $3 -eq 0 ]] && return

  local GOTSIZE=$((`eval ls -l "$1".{1..$2} 2> /dev/null | awk 'BEGIN{SUM=0}{SUM=SUM+$5}END{print SUM}'`))
  local TIMEDIFF=$(( `date +%s` - $4 ))
  local RATE=$(( ($GOTSIZE / $TIMEDIFF)/1024 ))
  local PCT=$(( ($GOTSIZE*100) / $3 ))

  echo "Downloading $1 in $2 parts: $(($GOTSIZE/1048576)) / $(($3/1048576)) mb @ $(($RATE)) KB/s ($PCT%).    "

  return
}

function download() {
  local SRC="$1"
  local DEST="$2"
  local FILE_NAME=$3
  local SPLIT_NUM=1

  WRK_DIR="$DEST/wrk"
  [ -f "$WRK_DIR" ] || `mkdir -m 755 "$WRK_DIR"`

  curl --ftp-pasv -o "${WRK_DIR/$FILE_NAME}.1" "$SRC/$FILE_NAME" 2> /dev/null &
  if [ $? -eq 0 ]
  then
    echo "Download started"
  else
    echo "Could not download file: $?" >&2
    clean
  fi

  # Wait for the job to complete while showing progress
  TIME=$((`date +%s`-1))
  while jobs | grep -q Running ; do
    echo -n $(calcProgress "${WRK_DIR/$FILE_NAME}.1" $SPLIT_NUM $FILE_SIZE $TIME)
    tput ech ${#FILE_SIZE} # erase the last reported size value
    tput cub 1000 # move 1000 chars left
    sleep 1
  done

  echo $(calcProgress "${WRK_DIR/$FILE_NAME}.1" $SPLIT_NUM $FILE_SIZE $TIME)

  eval mv "${WRK_DIR/$FILE_NAME}.1" "$DEST/$FILE_NAME"

  if [ $? -eq 0 ]
  then
    echo "Download completed! Check file in $DEST"
    local file_path=$(echo ${SRC} | cut -d@ -f2 | cut -d/ -f2- | cut -d? -f1)
    echo "curl $SRC -Q\"DELE $file_path/$FILE_NAME\""
    #curl ftp://alex:ficus5657@192.168.1.1/ftp/root/alex -Q"DELE /ftp/root/alex/New.Text.Document.txt"
    [ $DELETE_REMOTE_FILE = "y" ] && `curl $SRC -Q"DELE $file_path/$FILE_NAME"`
  else
    echo "Could not download file: $?" >&2
    clean
  fi
}

function downloadInChunks() {
  local SRC="$1"
  local DEST="$2"
  local FILE_NAME=$3
  local FILE_SIZE=$4
  local SPLIT_SIZE=$5

  SPLIT_NUM=$((${FILE_SIZE:-0}/$SPLIT_SIZE))
  [ $SPLIT_NUM -ne 0 ] || SPLIT_NUM=1

  WRK_DIR="$DEST/wrk"
  [ -f "$WRK_DIR" ] || `mkdir -m 755 "$WRK_DIR"`

  local CURL_RANGE_ARG="--range $START-$END"
  [ $SPLIT_NUM -eq 1 ] || CURL_RANGE_ARG=""

  local START=0
  local CHUNK=$((${FILE_SIZE:-0}/${SPLIT_NUM:-1}))
  local END=$CHUNK

  #Invoke curls
  for PART in `eval echo {1..$SPLIT_NUM}`;do
    curl --ftp-pasv -o "$WRK_DIR/$FILE_NAME.$PART" --range $START-$END "$SRC/$FILE_NAME" 2> /dev/null &
    START=$(($START+$CHUNK+1))
    END=$(($START+$CHUNK))
  done

  #Wait for all parts to complete while spewing progress
  TIME=$((`date +%s`-1))
  while jobs | grep -q Running ; do
    echo -n $(calcProgress "$WRK_DIR/$FILE_NAME" $SPLIT_NUM $FILE_SIZE $TIME)
    tput ech ${#FILE_SIZE} # erase the last reported size value
    tput cub 1000 # move 1000 chars left
    sleep 1
  done

  echo $(calcProgress "$WRK_DIR/$FILE_NAME" $SPLIT_NUM $FILE_SIZE $TIME)

  #Join all the parts
  eval cat "$WRK_DIR/$FILE_NAME".{1..$SPLIT_NUM} > "$DEST/$FILE_NAME"

  if [ $? -eq 0 ]
  then
    echo "Download completed! Check file $DEST"
  else
    echo "Could not download file: $?" >&2
    clean
  fi

  return
}

function print() {
  echo "$1 [$2]"
}


function check() {
  if [ $1 -eq 0 ]
    then
      echo "Download started"
    else
      echo "Could not download file: $1" >&2
      clean
  fi
}


###########################
# main
###########################

URL=$1
OUT_DIR=${2:-"/tmp"}
DELETE_REMOTE_FILE=${3:-"n"}
CUT_OFF_TIME="$(date -d "$4" '+%s')" # time in epoch milliseconds
SPLIT_SIZE=${5:-128}
SPLIT_SIZE=$(($SPLIT_SIZE * 1024 * 1024)) # convert to mb

echo $URL
echo $OUT_DIR
echo $CUT_OFF_TIME
echo $SPLIT_SIZE

#Check parameters
if [ ! "$1" ] ; then
  echo "source url with the directory is required"
  usage
fi

if [ ! "$2" ] ; then
  echo "target dir is required"
  usage
fi

if [ ! "$3" ] ; then
  # use the timestamp of the last file we downloaded or if we cant find one assume a week from now
  LATEST=$(getDestinationLastModifiedDate $2)
  CUT_OFF_TIME=${LATEST:-`date --date 'last week' '+%s'`}
  print "Using last download time as cutoff marker" $CUT_OFF_TIME
fi

# add verbose mode
print "URL" $URL
print "OUT_DIR" $OUT_DIR
print "DELETE_REMOTE_FILE" $DELETE_REMOTE_FILE
print "CUT_OFF_TIME" "$(date -d "@$CUT_OFF_TIME")"
#print "SPLIT_SIZE" $SPLIT_SIZE

#exit

#Trap ctrl-c
trap clean SIGINT SIGTERM

declare -a files
getDirectoryContent files $URL
getNewFiles files $URL $CUT_OFF_TIME

for f in ${files[@]}; do
  FILE_SIZE=$(getSize "$URL/$f")
  echo "Downloading $f with size $FILE_SIZE"
#  downloadInChunks "$URL" "$OUT_DIR" "$f" $FILE_SIZE $SPLIT_SIZE
  download "$URL" "$OUT_DIR" "$f" $FILE_SIZE $SPLIT_SIZE
done

rm -R "$OUT_DIR/wrk"

# ./file-get ftp://alex:ficus5657@192.168.1.1/ftp/root/alex/ ./td
# curl ftp://alex:ficus5657@192.168.1.1/ftp/root/alex/New.Text.Document.txt -Q"DELE ftp://alex:ficus5657@192.168.1.1/ftp/root/alex/New.Text.Document.txt"