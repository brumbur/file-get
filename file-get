#!/bin/bash


###########################
# functions
###########################

# monitor given remote directory and download new files to the target directory
function usage {
  echo -e "Usage:\n\t`basename $0` [source url] [target dir] [cutoff time] [chunk size (default: 128MB)]"
  echo -e "Example:\n\t `basename $0` ftp://pesho:mzprx@192.168.1.1:21/remote/directory /tmp \"Mon, 17 Feb 2020 20:38:04 GMT\""
  echo -e "\n\t `basename $0` ftp://pesho:mzprx@192.168.1.1:21/remote/directory /tmp yesterday"
  exit 1
}

#Cleanup on kill
function clean {
  echo "Download failed! Cleaning..."
  # kill 0
  rm -R $OUT_DIR/wrk 2> /dev/null
  rm $OUT_DIR/$FILENAME.* 2> /dev/null
  exit 1
}

# get the file details and parse the size
function getSize() {
  local SIZE="`curl -qIL $1 2> /dev/null | awk '/Length/ {print $2}'|grep -o '[0-9]*'`"
  local SIZE=${SIZE:-1}
  echo $SIZE
  return
}

# takes an array refference and updates it with the file names in the given directory
function getDirectoryContent() {
  local -n arr=$1
  local url="$2/"

  arr=($(curl -s "$url" | sed -nr 's/(^-[ rw-].*)( *)([0-9])(.*)/\4/p' | sed -e 's/^ //g' | sed -e 's/ /\ /g'))
  return
}

# takes the complete URL as argument and returns the last modified time
function getLastModifiedDate() {
    local dt=`curl -I "$1" 2>&1 | awk '/Last-Modified:/ {sub("\r",""); print  $2,$3,$4,$5,$6,$7}'`
    local ms="$(date -d "${dt}" '+%s')" # convert to ms
    echo "$ms"
    return
}

# filters out any files that are older than the cutoff time
function getNewFiles() {
    local -n arr=$1
    local url=$2
    local cutoff=$3

    for f in ${arr[@]}; do
        local lmd=$(getLastModifiedDate "$url/$f")
        if [ $cutoff -gt $lmd ]; then
            echo "$f is old, skipping it"git c
            arr=("${arr[@]/$f}") # remove old file
        fi
    done
    return
}

function calcProgress() {
  # local FNAME=$1
  # local PARTNUM=$2
  # local SZ=$3
  # local T=$4

  local GOTSIZE=$((`eval ls -l "$1".{1..$2} 2> /dev/null | awk 'BEGIN{SUM=0}{SUM=SUM+$5}END{print SUM}'`))
  local TIMEDIFF=$(( `date +%s` - $4 ))
  local RATE=$(( ($GOTSIZE / $TIMEDIFF)/1024 ))
  local PCT=$(( ($GOTSIZE*100) / $3 ))

  echo "Downloading $1 in $2 parts: $(($GOTSIZE/1048576)) / $(($3/1048576)) mb @ $(($RATE)) KB/s ($PCT%).    "

  return
}

function download() {
  local SRC="$1"
  local DEST="$2"
  local FILE_NAME=$3
  local FILE_SIZE=$4
  local SPLIT_SIZE=$5

  SPLIT_NUM=$((${FILE_SIZE:-0}/$SPLIT_SIZE))
  [ $SPLIT_NUM -ne 0 ] || SPLIT_NUM=1

  WRK_DIR="$DEST/wrk"
  [ -f "$WRK_DIR" ] || `mkdir -m 755 "$WRK_DIR"`

  local CURL_RANGE_ARG="--range $START-$END"
  [ $SPLIT_NUM -eq 1 ] || CURL_RANGE_ARG=""

  local START=0
  local CHUNK=$((${FILE_SIZE:-0}/${SPLIT_NUM:-1}))
  local END=$CHUNK

  #Invoke curls
  for PART in `eval echo {1..$SPLIT_NUM}`;do
    # echo "curl --ftp-pasv -o $WRK_DIR/$FILE_NAME.$PART --range $START-$END $SRC/$FILE_NAME"
    curl --ftp-pasv -o "$WRK_DIR/$FILE_NAME.$PART" --range $START-$END "$SRC/$FILE_NAME" 2> /dev/null &
    START=$(($START+$CHUNK+1))
    END=$(($START+$CHUNK))
  done

  #Wait for all parts to complete while spewing progress
  TIME=$((`date +%s`-1))
  while jobs | grep -q Running ; do
    echo -n $(calcProgress "$WRK_DIR/$FILE_NAME" $SPLIT_NUM $FILE_SIZE $TIME)
    tput ech ${#FILE_SIZE} # erase the last reported size value
    tput cub 1000 # move 1000 chars left
    sleep 1
  done

  echo $(calcProgress "$WRK_DIR/$FILE_NAME" $SPLIT_NUM $FILE_SIZE $TIME)

  #Join all the parts
  eval cat "$WRK_DIR/$FILE_NAME".{1..$SPLIT_NUM} > "$DEST/$FILE_NAME"

  if [ $? -eq 0 ]
  then
    echo "Download completed! Check file $DEST"
    rm -R $WRK_DIR
  else
    echo "Could not download file: $?" >&2
    clean
  fi

  return
}


###########################
# main
###########################

URL=$1
OUT_DIR=${2:-OUT_DIR-"/tmp"}
CUT_OFF_TIME="$(date -d "$3" '+%s')" # time in epoch miliseconds
SPLIT_SIZE=${4:-128}
SPLIT_SIZE=$(($SPLIT_SIZE * 1024 * 1024)) # convert to mb

echo $URL
echo $OUT_DIR
echo $CUT_OFF_TIME
echo $SPLIT_SIZE

#Check parameters
if [ ! "$1" ] ; then
  echo "source url with the directory is required"
  usage
fi

if [ ! "$2" ] ; then
  echo "target dir is required"
  usage
fi

if [ ! "$3" ] ; then
  echo "Cut-off time is required"
  usage
fi

#Trap ctrl-c
trap clean SIGINT SIGTERM

echo "CUT_OFF_TIME $CUT_OFF_TIME"

declare -a files
getDirectoryContent files $URL
getNewFiles files $URL $CUT_OFF_TIME
echo "Found ${#files[@]} files"

for f in ${files[@]}; do
  FILE_SIZE=$(getSize "$URL/$f")
  echo "Downloading $f with size $FILE_SIZE"
  download "$URL" "$OUT_DIR" "$f" $FILE_SIZE $SPLIT_SIZE
done